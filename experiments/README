This directory archives Fast Downward experiments.

Experiments using lab
=====================
All experiments use their own common_setup.py module that simplifies
the creation of experiments. We recommend using common_setup.py from
issue392 for new experiments since it is the most polished version and
provides the greatest flexibility.

Below we list some experiments that show how certain tasks can be
performed:

* Test changes that affect the whole planner:
  * issue422/issue422.py shows the general setup but tests only LM-cut.
    Use "from downward.configs import default_configs_optimal" and set
    the parameter suite to default_configs_optimal. For satisficing
    configs use default_configs_satisficing and
    suite_satisficing_with_ipc11.

* Add a custom log parser:
  * issue469/issue469.py adds a parser for raw_memory

* Add scatter plots for custom attributes:
  * issue214/issue214.py

* Run configurations on the same tasks multiple times to reduce noise:
  * issue420/issue420-v1-regressions.py

* Independent CompareRevisionReports for portfolio configs and core solvers:
  * issue462/issue462-opt.py


Microbenchmarks
=================

Some experiments don't run the whole planner or planner components,
but just contain small microbenchmarks for particular functionality.
These include their own little self-contained codebases and may be a
good starting point for similar microbenchmarks. Examples:

* Benchmarking of random number generation:
  * issue269/rng-microbenchmark

If you add your own microbenchmark, it is recommended to start from a
copy of an existing example and follow the naming convention
issue[...]/[...]-microbenchmark for the code. This way, .hgignore
should be set up correctly out of the box.
