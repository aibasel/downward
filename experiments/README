This directory archives Fast Downward experiments.

Experiments using lab
=====================
All experiments use their own common_setup.py module that simplifies
the creation of experiments. We recommend using common_setup.py from
issue546 for new experiments since it is the most polished version and
provides the greatest flexibility.

Previously, most experiments imported lab's downward/configs.py module.
To make experiments more self-contained, the module has been deprecated
and we recommend either copying config.py into the experiment
directory (see issue77 for an example) or manually specifying the
configurations (see issue511).

Below we list some experiments that show how certain tasks can be
performed:

* Test changes that affect the whole planner:
  * issue422/issue422.py shows the general setup but tests only LM-cut.
    Use "from downward.configs import default_configs_optimal" and set
    the parameter suite to default_configs_optimal. For satisficing
    configs use default_configs_satisficing and
    suite_satisficing_with_ipc11.

* Add a custom log parser:
  * issue439/regressions.py

* Compare all attributes for tasks where we lose coverage:
  * issue439/regressions.py

* Use custom time limit:
  * issue439/issue439.py

* Add scatter plots for custom attributes:
  * issue214/issue214.py

* Run configurations on the same tasks multiple times to reduce noise:
  * issue420/issue420-v1-regressions.py

* Independent CompareRevisionReports for portfolio configs and core solvers:
  * issue462/issue462-opt.py

* RelativeScatterPlotReport:
  * issue77/issue77-v7-opt.py

* M&S: configs and additional parsing:
  * issue548/base-v2.py and issue548/ms-parser.py


Microbenchmarks
===============

Some experiments don't run the whole planner or planner components, but
just contain small microbenchmarks for particular functionality. These
may be a good starting point for similar microbenchmarks. Examples:

* Benchmarking of random number generation:
  * issue269/rng-microbenchmark

If you add your own microbenchmark, it is recommended to start from a
copy of an existing example and follow the naming convention
issue[...]/[...]-microbenchmark for the code. This way, .hgignore
should be set up correctly out of the box.
