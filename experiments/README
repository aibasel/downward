This directory archives Fast Downward experiments.

Experiments using Downward Lab
==============================

All experiments use their own common_setup.py module that simplifies the
creation of experiments. We recommend using common_setup.py (and related
*.py files) from issue839 for new experiments since it is the most
recent version.

Below we list some experiments that show how certain tasks can be
performed. Some of them use Downward Lab's deprecated
DownwardExperiment:

* Test changes that affect the whole planner:
  * issue481/v1-*.py show the general setup. You will need different
    experiments for satisficing, optimal and anytime configurations.

* Add a custom log parser:
  * issue735/v1.py

* Compare all attributes for tasks where we lose coverage:
  * issue439/regressions.py

* Use custom time limit:
  * issue439/issue439.py

* Add scatter plots for custom attributes:
  * issue214/issue214.py

* Run configurations on the same tasks multiple times to reduce noise:
  * issue420/issue420-v1-regressions.py

* Independent CompareRevisionReports for portfolio configs and core solvers:
  * issue462/issue462-opt.py

* RelativeScatterPlotReport:
  * issue77/issue77-v7-opt.py

* M&S: configs and additional parsing and a check for availability of matplotlib
  * issue849/v1.py and issue849/ms-parser.py

* Run experiment in debug mode (with assertions):
  * issue650/v2.py

* Translator:
  * issue671/v1.py runs simple configs on the entire set of suites and
    reports also translator-specific attributes

* Translator-only experiments:
  * issue733/v1.py compares translator time and memory under Python 2 and 3
  * issue869/base-translate-all.py translates all benchmarks


Microbenchmarks
===============

Some experiments don't run the whole planner or planner components, but
just contain small microbenchmarks for particular functionality. These
may be a good starting point for similar microbenchmarks. Examples:

* Benchmarking of random number generation:
  * issue269/rng-microbenchmark

If you add your own microbenchmark, it is recommended to start from a
copy of an existing example and follow the naming convention
issue[...]/[...]-microbenchmark for the code. This way, .hgignore
should be set up correctly out of the box.
